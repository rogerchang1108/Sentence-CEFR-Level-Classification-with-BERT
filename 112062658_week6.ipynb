{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7L3cYlZl15_K"
      },
      "source": [
        "# Sentence Level Classification with BERT\n",
        "\n",
        "Our goal is to train a classifier that can predict the CEFR level of any given sentence. In this notebook we will use ðŸ¤—[Hugging Face](https://huggingface.co/) and its transformers library as the training framework, with [Pytorch](https://pytorch.org/) as the deep learning backend.\n",
        "\n",
        "There is a dataset containing sentences with the corresponding CEFR level, and we have to use BERT and train a sentence classifier with this dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G8TbxtroCxM8"
      },
      "source": [
        "## Prepare your environment\n",
        "\n",
        "As always, we install all packages with a virtual environment manager, like [venv](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) or [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html), to prevent version conflicts of different packages.  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wSHP0CPoXj7Z"
      },
      "source": [
        "### Install CUDA\n",
        "Deep learning is a computionally extensive process. It takes lots of time if relying only on the CPU, especially when it's trained on a large dataset. That's why using GPU instead is generally recommended.  \n",
        "To use GPU for computation, you have to install [CUDA toolkit](https://developer.nvidia.com/cuda-toolkit) as well as the [cuDNN library](https://developer.nvidia.com/cudnn) provided by NVIDIA.  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f78jLXeZfPyH"
      },
      "source": [
        "\n",
        "### Install python packages\n",
        "The following python packages will be used in this tutorial:\n",
        "\n",
        "1. `numpy`: for matrix operation\n",
        "2. `scikit-learn`: for label encoding\n",
        "3. `datasets`: for data preparation\n",
        "4. `transformers`: for model loading and finetuing\n",
        "5. `pytorch`: the backend DL framework\n",
        "  - Note that the pt version must support the CUDA version you've installed if you want to use GPU."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3cS9CxjyfQ-F"
      },
      "source": [
        "### Select GPU(s) for your backend\n",
        "\n",
        "Skip this section if you have no intension of using GPU with tensorflow/pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sEJ8Y8SCfWp_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# select your GPU. Note that this should be set before you load tensorflow or pytorch.\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ['WANDB_DISABLE'] = 'True'\n",
        "\n",
        "# To use multiple GPUs, combine all GPU ID with commas\n",
        "# e.g. >>> os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1p_qQKbfcCH",
        "outputId": "d25bdd87-959c-435d-8c57-22f5c350098f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.1+cu118\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "# Check if any GPU is used\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-P0foxjBDQSu"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "Before starting the training, we need to load and process our dataset - but wait, let's decide which model we want to use first.  \n",
        "\n",
        "In the highly unlikely chance you've never heard of it, [BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) is a language model proposed by Google AI in 2018, and it's currently one of the most popular models used in NLP.  \n",
        "You can learn more about it here:\n",
        "- [BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/) by Samia, 2019.\n",
        "\n",
        "\n",
        "However, we will not directly use BERT in this tutorial, because it's large and takes too long to train. Instead, we'll be using [DistilBert](https://medium.com/huggingface/distilbert-8cf3380435b5), a version of BERT that while light-weight, reserves 95% of its original accuracy.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Lqb2cHCmDIEp"
      },
      "outputs": [],
      "source": [
        "# the model you want to use. Available models can be found here: https://huggingface.co/models\n",
        "MODEL_NAME = 'distilbert-base-uncased'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QxSUKsTkDSxJ"
      },
      "source": [
        "### Load data\n",
        "\n",
        "Similar to the `transformers` library, `datasets` is also a package by huggingface. It contains many public datasets online and can help us with the data processing.  \n",
        "We can use `load_dataset` function to read the input `.csv` file provided for this assignment.\n",
        "\n",
        "Reference:\n",
        " - [Official datasets document](https://huggingface.co/docs/datasets)\n",
        " - [datasets.load_dataset](https://huggingface.co/docs/datasets/loading.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hjY9HMNIt5jf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset csv (/home/nlplab/roger/.cache/huggingface/datasets/csv/data-be30cc0ec9fc77bb/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.011392831802368164,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 2,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34c5e08bcec344ad85f5e00e4e42cbaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'level'],\n",
              "        num_rows: 20720\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'level'],\n",
              "        num_rows: 2300\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the data using the load_dataset function\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('./data')\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npepcv7GfMHI",
        "outputId": "02ed2940-370f-444c-b748-abcd623891bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'level'],\n",
            "    num_rows: 20720\n",
            "})\n",
            "{'text': 'You can contact me by e-mail.', 'level': 'A1'}\n",
            "['My mother is having her car repaired.', 'You can contact me by e-mail.', 'He had a break for the weekend, and he called me: \"I am in London, so, if you want to see me, it\\'s the time!\"', \"Research shows that 40 percent of the program's viewers are aged over 55.\", \"I'd guess she's about my age.\"]\n"
          ]
        }
      ],
      "source": [
        "print(dataset['train'])\n",
        "print(dataset['train'][1])\n",
        "print(dataset['train']['text'][:5])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E3olKw19uuJQ"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "As always, texts should be tokenized, embedded, and padded before being put into the model.  \n",
        "But not to worry, there are libraries from huggingface to help with this, too."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x3ANJAV7wn2R"
      },
      "source": [
        "#### Sentence processing\n",
        "\n",
        "Different pre-trained language models may have their own preprocessing models, and that's why we should use the tokenizers trained along with that model. In our case, we are using distilBERT, so we should use the distilBERT tokenizer.  \n",
        "\n",
        "With huggingface, loading different tokenizers is extremely easy: just import the AutoTokenizer from `transformers` and tell it what model you plan to use, and it will handle everything for you.\n",
        "\n",
        "Reference:\n",
        " - [transformers.AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DALrmSh6wpmy"
      },
      "outputs": [],
      "source": [
        "# load the distilBERT tokenizer using AutoTokenizer\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd43REmNxCOV"
      },
      "source": [
        "#### Label processing\n",
        "\n",
        "Our labels also need to be processed, so let's do that next.\n",
        "\n",
        "For this tutorial, we'll use the OneHotEncoder provided by scikit-learn.\n",
        "\n",
        "For now, just declare a new encoder and use `fit` to learn the data. Hint: you should still end up with 6 labels.\n",
        "\n",
        "Documents:\n",
        " - [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q3ml0_WxxFh5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\n",
              "              handle_unknown='error', sparse=True)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# declare a new encoder and let it learn from the dataset\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "encoder.fit(numpy.array(dataset['train']['level']).reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRY2GDJa1MxF",
        "outputId": "df210570-a2f8-478d-8902-e6f6c4f33af5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "# check if you still have 6 labels\n",
        "LABEL_COUNT = len(encoder.categories_[0])\n",
        "print(LABEL_COUNT)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qrfO8c4R1eWO"
      },
      "source": [
        "#### Process the data\n",
        "\n",
        "To make things easier, we can write a function to process our dataset in batches. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cvwnYLah1dbN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def preprocess(dataslice): # dataset['train'][:10]\n",
        "    \"\"\" Input: a batch of your dataset\n",
        "        Example: { 'text': [['sentence1'], ['setence2'], ...],\n",
        "                   'label': ['label1', 'label2', ...] }\n",
        "    \"\"\"\n",
        "    \n",
        "    # use your tokenizor and encoder to get sentence embeddings and encoded labels\n",
        "    # print(dataslice['text'])\n",
        "    \n",
        "    dataslice['input_ids'] = []\n",
        "    dataslice['attention_mask'] = []\n",
        "    dataslice['label'] = []\n",
        "    \n",
        "    for idx in range(len(dataslice['text'])):\n",
        "        text = dataslice['text'][idx]\n",
        "        level = dataslice['level'][idx]\n",
        "        \n",
        "        t = tokenizer(text)\n",
        "        l = encoder.transform([[level]]).toarray()\n",
        "        \n",
        "        dataslice['input_ids'].append(t['input_ids'])\n",
        "        dataslice['attention_mask'].append(t['attention_mask'])\n",
        "        dataslice['label'].append(np.reshape(l, (6)))\n",
        "        \n",
        "    \"\"\" Output: a batch of processed dataset\n",
        "        Example: { 'input_ids': ...,\n",
        "                   'attention_masks': ...,\n",
        "                   'label': ... }\n",
        "    \"\"\"\n",
        "    \n",
        "    return dataslice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Nr0-Y1bR2efQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /home/nlplab/roger/.cache/huggingface/datasets/csv/data-be30cc0ec9fc77bb/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-13c7fb7d4be74de6.arrow\n",
            "Loading cached processed dataset at /home/nlplab/roger/.cache/huggingface/datasets/csv/data-be30cc0ec9fc77bb/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-2dde0fa24c7de972.arrow\n"
          ]
        }
      ],
      "source": [
        "# map the function to the whole dataset\n",
        "processed_data = dataset.map(preprocess,    # your processing function\n",
        "                             batched = True # Process in batches so it can be faster\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G5ftJbjfa4i",
        "outputId": "c88d0fc1-85ca-4cdb-e191-a426e29fcfc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'level', 'input_ids', 'attention_mask', 'label'],\n",
            "        num_rows: 20720\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'level', 'input_ids', 'attention_mask', 'label'],\n",
            "        num_rows: 2300\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'text': 'My mother is having her car repaired.',\n",
              " 'level': 'B1',\n",
              " 'input_ids': [101, 2026, 2388, 2003, 2383, 2014, 2482, 13671, 1012, 102],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'label': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(processed_data)\n",
        "processed_data['train'][0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G9z7ZMtP22b9"
      },
      "source": [
        "### DataCollator\n",
        "\n",
        "You might have noticed that we skipped padding the sentences. That's because we are going to do it during training.  \n",
        "\n",
        "To do training-time processing, we can use the DataCollator Class provided by `transformers`. And guess what - transformers has a class that will handle padding for us, too!\n",
        "\n",
        " - [transformers.DataCollatorWithPadding](https://huggingface.co/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x5orGYjN39dz"
      },
      "outputs": [],
      "source": [
        "# declare a collator to do padding during traning\n",
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "di75QVgv4V81"
      },
      "source": [
        "## Training\n",
        "\n",
        "Finally, we can move on to training."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "53dO3pg85u0n"
      },
      "source": [
        "### Preparation\n",
        "\n",
        "We can load the pretrained model from `transformers`.  \n",
        "Generally, you need to build your own model on top of BERT if you want to use BERT for some downstream tasks, but again, sequence classification is a popular topic. With the support from `transformers` library, it can be done in two lines of codes: \n",
        "\n",
        "1. Load `AutoModelForSequenceClassification` Class.\n",
        "2. Load the pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UyDyv7wp5qdD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
        "                                                           num_labels = LABEL_COUNT)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DtmP4TEh6XiB"
      },
      "source": [
        "#### Split train/val data\n",
        "\n",
        "The `Dataset` class we prepared before has a `train_test_split` method. You can use it to split your (processed) dataset.\n",
        "\n",
        "Document:\n",
        " - [datasets.Dataset - Sort, shuffle, select, split, and shard](https://huggingface.co/docs/datasets/process.html#sort-shuffle-select-split-and-shard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NnbD1KW16YWn"
      },
      "outputs": [],
      "source": [
        "# choose a validation size and split your data\n",
        "train_val_dataset = processed_data['train'].train_test_split(test_size=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u93fCpofgbe",
        "outputId": "65b7044c-6a01-4c07-f456-75d0e83d9d70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'level', 'input_ids', 'attention_mask', 'label'],\n",
            "        num_rows: 16576\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'level', 'input_ids', 'attention_mask', 'label'],\n",
            "        num_rows: 4144\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(train_val_dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YzmSbOaD7O0F"
      },
      "source": [
        "#### Setup training parameters\n",
        "\n",
        "We are using the TrainerAPI to do the training. Trainer is yet another utility provided by huggingface, which helps you train the model with ease.  \n",
        "\n",
        "Document:\n",
        "- [transformers.TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments)\n",
        "- [transformers.Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ABqlinlO76Ax"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5lzXTG1y7q7n"
      },
      "outputs": [],
      "source": [
        "# set and tune your training properties\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "OUTPUT_DIR = './'\n",
        "LEARNING_RATE = 5e-5\n",
        "BATCH_SIZE = 100\n",
        "EPOCH = 5\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = OUTPUT_DIR,\n",
        "    learning_rate = LEARNING_RATE,\n",
        "    per_device_train_batch_size = BATCH_SIZE,\n",
        "    per_device_eval_batch_size = BATCH_SIZE,\n",
        "    num_train_epochs = EPOCH,\n",
        "    report_to = 'none'\n",
        "    # you can set more parameters here if you want\n",
        ")\n",
        "\n",
        "# now give all the information to a trainer\n",
        "trainer = Trainer(\n",
        "    # set your parameters here\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = train_val_dataset['train'],\n",
        "    eval_dataset = train_val_dataset['test']\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gE0DpS3s7rhg"
      },
      "source": [
        "### Training\n",
        "\n",
        "This is the easy part. Simply ask the trainer to train the model for you!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wsrQOyJCiFas"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='830' max='830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [830/830 03:26, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.315800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=830, training_loss=0.25925249306552384, metrics={'train_runtime': 207.4767, 'train_samples_per_second': 399.467, 'train_steps_per_second': 4.0, 'total_flos': 1187596984846464.0, 'train_loss': 0.25925249306552384, 'epoch': 5.0})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4JBUA0pe-S9b"
      },
      "source": [
        "### Save for future use\n",
        "\n",
        "Hint: try using `save_pretrained`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e30uXCf-cXc",
        "outputId": "f003dfa4-060a-4419-9ba6-f559029e5402"
      },
      "outputs": [],
      "source": [
        "# practice saving your model for future use\n",
        "model.save_pretrained(\"./\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7QSZjlcG9fOk"
      },
      "source": [
        "## Prediction\n",
        "\n",
        "Now we know exactly how to train a model, but how do we use it for predicting results?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O7akP5Hh9ugG"
      },
      "source": [
        "### Load finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Lfb_zJGm9vJP"
      },
      "outputs": [],
      "source": [
        "# load the model that you saved\n",
        "mymodel = AutoModelForSequenceClassification.from_pretrained('./')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Vs3iBE_nck"
      },
      "source": [
        "### Get the prediction\n",
        "\n",
        "Here are a few example sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FtLt6IBi_pLF"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    # A2\n",
        "    \"Remember to write me a letter.\",\n",
        "    # B2\n",
        "    \"Strawberries and cream - a perfect combination.\",\n",
        "    \"This so-called \\\"Perfect Evening\\\" was so disappointing, as well as discouraging us from coming to your Circle Theatre again.\",\n",
        "    # C1\n",
        "    \"Some may altogether give up their studies, which I think is a disastrous move.\",\n",
        "]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_xhk7ZRX_2U2"
      },
      "source": [
        "All we need to do is to transform them to embeddings, and then we can get predictions by calling your finetuned model.  \n",
        "\n",
        "Since we don't have a DataCollator to pad the sentence and do the matrix transformation this time, we have to pad and transform the matrice on our own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWONG_lCAkyN",
        "outputId": "0c408ae7-997d-4c0c-8f49-c6196397602f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.9237,  0.8402, -3.1462, -5.0538, -4.1780, -4.6199],\n",
              "        [-6.6779, -6.2395, -3.7227, -0.0955, -1.5378, -2.6053],\n",
              "        [-5.7536, -5.4781, -5.2673, -1.1857,  1.3211, -3.6174],\n",
              "        [-5.7480, -5.6256, -3.6208,  2.3783, -2.5812, -3.8506]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transform the sentences into embeddings\n",
        "input = tokenizer(examples, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "# Get the output\n",
        "logits = mymodel(**input).logits\n",
        "logits"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vBMUBD-1BFW1"
      },
      "source": [
        "Logits aren't very readable for us. Let's use softmax \n",
        "activation to transform them into more probability-like numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjiKxLaBBGah",
        "outputId": "006d24be-ed4f-48cb-ab2c-fa7b70b44dd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.4239e-01, 8.3086e-01, 1.5425e-02, 2.2899e-03, 5.4976e-03, 3.5336e-03],\n",
            "        [1.0273e-03, 1.5925e-03, 1.9730e-02, 7.4195e-01, 1.7539e-01, 6.0307e-02],\n",
            "        [7.7487e-04, 1.0207e-03, 1.2602e-03, 7.4655e-02, 9.1573e-01, 6.5610e-03],\n",
            "        [2.9215e-04, 3.3017e-04, 2.4515e-03, 9.8805e-01, 6.9328e-03, 1.9482e-03]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "predicts = nn.functional.softmax(logits, dim = -1)\n",
        "print(predicts)\n",
        "#torch.max(predicts, 1).indices"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zAqgoJTFBchb"
      },
      "source": [
        "#### Transform logits back to labels(10pts)\n",
        "\n",
        "Now you've got the output. Write a function to map it back into labels!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvcyotBfBifR",
        "outputId": "50cae16f-06ed-4f04-c684-496044f37233"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A2', 'B2', 'C1', 'B2']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# try to process the result\n",
        "def convertToLabels(logits):\n",
        "    d = {\n",
        "        0: 'A1',\n",
        "        1: 'A2',\n",
        "        2: 'B1',\n",
        "        3: 'B2',\n",
        "        4: 'C1',\n",
        "        5: 'C2'\n",
        "    }\n",
        "    return list(map(d.get, list(torch.max(logits, 1).indices.detach().numpy())))\n",
        "\n",
        "convertToLabels(predicts)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gfCuP95IBvP-"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Let's see how you did!  \n",
        "Load the testing data and calculate your accuracy.\n",
        "\n",
        "We want you to calculate the three kinds of accuracy mentioned in the lecture, which will also be explained in the following section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "C8hUc4sLCW40"
      },
      "outputs": [],
      "source": [
        "# load test data\n",
        "# preprocess\n",
        "# get predictions\n",
        "input_test = tokenizer(processed_data['test']['text'][:], truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "logits_test = mymodel(**input_test).logits\n",
        "predicts_test = nn.functional.softmax(logits_test, dim = -1)\n",
        "predict_label_test = convertToLabels(predicts_test)\n",
        "# transform predictions back into labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoC2EAHxCXdj",
        "outputId": "cc711f3c-c789-4d0f-d92f-adf04dc54432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C2: No longer a remote, backward, unimportant country, it became a force to be reckoned with in Europe.\n",
            "B2: Unfortunately he was too fast and I couldn't keep up with him.\n",
            "B2: Most mushrooms are totally harmless, but some are poisonous.\n",
            "C2: This provided solid evidence that he committed the crime.\n",
            "C1: You can't just accept everything you read in the newspapers at face value.\n",
            "A2: Remember to write me a letter.\n",
            "B1: She has long blond hair and blue eyes. She has a good figure.\n",
            "B2: Nowadays the aim in clothing is not just for covering and protecting ourselves.\n",
            "A2: Take two tablets, three times a day.\n",
            "C2: Well, you will be if you saw our slide show and talk - members can hardly forget that relaxing afternoon when we unfolded the sails on the lake and enjoyed the tranquility of the area.\n"
          ]
        }
      ],
      "source": [
        "#  try printing out some predictions to check if the outputs are reasonable and if you need to adjust your model at the end of every step.\n",
        "\n",
        "for idx, (sent, level) in enumerate(zip(processed_data['test']['text'], predict_label_test)):\n",
        "    if idx >= 10: break\n",
        "    print(f'{level}: {sent}') "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PBnEzAFhC7ZN"
      },
      "source": [
        "### Six Level Accuracy(30pts)\n",
        "\n",
        "Exact accuracy is probably what you're most familiar with:\n",
        "\n",
        "$\n",
        "accuracy = \\frac{\\#exactly\\:the\\:same\\:levels}{\\#total}\n",
        "$\n",
        "\n",
        "Example:\n",
        "```\n",
        "Prediction:   A1 A2 B1 B2 C1 C2\n",
        "Ground truth: A2 B1 B1 B2 B2 C2\n",
        "                    ^  ^     ^\n",
        "```\n",
        "\n",
        "The six level accuracy is $\\frac{3}{6} = 0.5$\n",
        "\n",
        "As the requirement, <u>your exact accuracy should be higher than $0.5$</u>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR2oVECyC8vD",
        "outputId": "72cebbe0-5ada-446b-b5f7-6a9849292792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5230434782608696\n"
          ]
        }
      ],
      "source": [
        "# calculate accuracy\n",
        "correct = 0\n",
        "size = len(processed_data['test']['level'])\n",
        "for idx in range(size):\n",
        "    if (predict_label_test[idx] == processed_data['test']['level'][idx]):\n",
        "        correct += 1\n",
        "\n",
        "print(correct / size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "We865ayVC_N7"
      },
      "source": [
        "### Three Level Accuracy(30pts)\n",
        "\n",
        "Three Level Accuracy is used when you only want a more general sense of right or wrong.\n",
        "\n",
        "$\n",
        "accuracy = \\frac{\\#the\\:same\\:ABC\\:levels}{\\#total}\n",
        "$\n",
        "\n",
        "Example:\n",
        "```\n",
        "Prediction:   A1 A2 B1 B2 C1 C2\n",
        "Ground truth: A2 B1 B1 B2 B2 C2\n",
        "              ^     ^  ^     ^\n",
        "```\n",
        "\n",
        "The three level accuracy is $\\frac{4}{6} = 0.667$\n",
        "\n",
        "As the requirement, <u>your exact accuracy should be higher than $0.6$</u>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCAKM9MRDCBk",
        "outputId": "f599130e-de22-4b28-a521-f378afd52711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7217391304347827\n"
          ]
        }
      ],
      "source": [
        "# calculate accuracy\n",
        "correct = 0\n",
        "for idx in range(size):\n",
        "    if (predict_label_test[idx][0] == processed_data['test']['level'][idx][0]):\n",
        "        correct += 1\n",
        "\n",
        "print(correct / size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3YSo46NX7nvb"
      },
      "source": [
        "### Fuzzy accuracy(30pts)\n",
        "\n",
        "However, the level of a sentence is relatively subjective. Generally speaking, $\\pm1$ errors are allowed in the real evaluation in linguistic area.  \n",
        "\n",
        "For example, if the actual label is 'B1', we'll also consider the prediction 'right' if the model predicts 'B2' or 'A2'.\n",
        "\n",
        "Hence, the fuzzy accuracy is\n",
        "\n",
        "$\n",
        "accuracy = \\frac{\\#good\\:enough\\:answers}{\\#total}\n",
        "$\n",
        "\n",
        "Example:\n",
        "```\n",
        "Prediction:   0 1 2 3 4 5\n",
        "Ground truth: 0 1 1 3 3 3\n",
        "              ^ ^ ^ ^ ^\n",
        "```\n",
        "\n",
        "The fuzzy accuracy is $\\frac{5}{6} = 0.833$\n",
        "\n",
        "As the requirement, <u>your accuracy should be higher than $0.8$</u>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27fP0BTc73Al",
        "outputId": "e792e699-4e04-4a82-ebff-d133aead0b60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8630434782608696\n"
          ]
        }
      ],
      "source": [
        "# calculate accuracy\n",
        "l = ['dummy', 'A1', 'A2', 'B1', 'B2', 'C1', 'C2', 'dummy']\n",
        "\n",
        "correct = 0\n",
        "for idx in range(size):\n",
        "    arr_index = l.index(processed_data['test']['level'][idx])\n",
        "    if (predict_label_test[idx] in l[arr_index-1:arr_index+2]):\n",
        "        correct += 1\n",
        "\n",
        "print(correct / size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aL7CAtRQbR7s"
      },
      "source": [
        "## Appendix "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lqqUYYPEanAF"
      },
      "source": [
        "\n",
        "<a name=\"Appendix-1-Install-CUDA\"></a>\n",
        "\n",
        "### Appendix 1 - Install CUDA\n",
        "\n",
        "1. Check your GPU vs. CUDA compatibility:\n",
        "   - [NVIDIA -> Your GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) -> GeForce and TITAN Products\n",
        "2. Check library vs. CUDA compatibility: \n",
        "   - Pytorch: [Previous PyTorch Versions](https://pytorch.org/get-started/previous-versions/)\n",
        "   - Tensorflow: [Linux/MacOX](https://www.tensorflow.org/install/source#tested_build_configurations) or [Windows](https://www.tensorflow.org/install/source_windows#tested_build_configurations)\n",
        "3. Note the highest CUDA version that fits your system.\n",
        "\n",
        "#### >> for conda/mamba users\n",
        "\n",
        "You can directly install CUDA library with the selected CUDA version.\n",
        "1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n",
        "2. `conda/mamba install -c conda-forge cudatoolkit=${VERSION}`\n",
        "\n",
        "#### >> for non-conda users\n",
        "\n",
        "1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n",
        "2. Download and install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)\n",
        "3. Download and install [cuDNN Library](https://developer.nvidia.com/rdp/cudnn-archive)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8hFAM1Cya4_c"
      },
      "source": [
        "### Appendix 2 - Further Readings\n",
        "\n",
        "1. [Huggingface Official Tutorials](https://github.com/huggingface/notebooks/tree/master/examples)\n",
        "2. How to use Bert with other downstream tasks: [How to use BERT from the Hugging Face transformer library](https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209): \n",
        "3. Training with pytorch backend: [transformers-tutorials](https://github.com/abhimishra91/transformers-tutorials)\n",
        "4. A more complicated example that include manual data/training processing with Pytorch: [Transformers for Multi-Label Classification made simple](https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1)\n",
        "5. [Text Classification with tensorflow](https://github.com/huggingface/notebooks/blob/master/examples/text_classification-tf.ipynb): tensorflow example"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
